{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arj_zcu9xTuK"
   },
   "source": [
    "## Credit Card Default Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGAbFXGZxTuO"
   },
   "outputs": [],
   "source": [
    "### importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhFWoJQoxTuQ"
   },
   "source": [
    "### Importing the dataset\n",
    "1. My dataset is in CSV file, and we will pandas read_csv to load the data.\n",
    "2. Checking the complete information.\n",
    "3. We will check the null values and deal with it accordingly.\n",
    "4. Describe the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "Xxcr2Wu1xTuR",
    "outputId": "807e735e-3720-43bd-ac44-791f5ffdf717",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('UCI_Credit_Card.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fJkDeskxTuS",
    "outputId": "453b7a41-b2b9-42a4-cba1-9602df7005f3"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t325PIAFxTuT"
   },
   "source": [
    "### THere are no null values in our dataset, and every column is numeric in nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FI8JLjG0xTuT",
    "outputId": "8f0fe380-3903-4842-e39b-c588f178b26b"
   },
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24Owix6YxTuV"
   },
   "source": [
    "### In this we can see that, marriage and education column contains, an extra unique value, if we go by the dataset, given on this link.\n",
    "### https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset\n",
    "### So I will rectify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hax1LOvYxTuW",
    "outputId": "23c7a392-1305-4815-9942-39d6ef245f97"
   },
   "outputs": [],
   "source": [
    "data['default.payment.next.month'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Y2KVxXNxTuY"
   },
   "source": [
    "### This is a imbalanced data, we will deal with this later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BUjPdPoxTuZ",
    "outputId": "1f130e6d-0598-4711-c870-7afce2f5be04"
   },
   "outputs": [],
   "source": [
    "data['EDUCATION'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqSDqCaHxTuZ"
   },
   "source": [
    "### I will convert 6 and 0 values to 5, as these represents unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVy_0rkBxTua"
   },
   "outputs": [],
   "source": [
    "data['EDUCATION'].replace({6:5, 0:5}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utmABKZGxTua",
    "outputId": "b296fe23-4810-4df0-a086-f5dde7820a5e"
   },
   "outputs": [],
   "source": [
    "data['MARRIAGE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PR8_SKeixTub"
   },
   "source": [
    "### I will convert 0 to 3, as it represents others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VxiQp8OxTub"
   },
   "outputs": [],
   "source": [
    "data['MARRIAGE'].replace({0:3}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tC0x0nt8xTub"
   },
   "source": [
    "## EDA\n",
    "### 1. Dropping column ID.\n",
    "### 2. Checking the relationships between variables through heatmap ans pair plot.\n",
    "### 3. Checking the distributions of some columns.\n",
    "### 3. Creating a new Dataframe, with columns like Sex, Education, Marriage, and Default, to perform visualizations after converting them into categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbmekFCtxTuc"
   },
   "outputs": [],
   "source": [
    "data.drop(['ID'], axis = 1, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sOKixEexTuc",
    "outputId": "03fb2b7a-40dd-451b-d4c2-6f1fadddc699"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ynr9uwH9xTuc",
    "outputId": "e036ea91-4997-4a61-9ed6-e723a0ab89ff"
   },
   "outputs": [],
   "source": [
    "relationDataFrame = pd.DataFrame(data, columns = ['LIMIT_BAL',\t'SEX',\t'EDUCATION',\t'MARRIAGE',\t'AGE',\t'PAY_0',\t'PAY_2'\n",
    "                                                  ,'BILL_AMT1',\t'BILL_AMT2',\t'BILL_AMT3',\t'BILL_AMT4','default.payment.next.month'])\n",
    "sns.pairplot(relationDataFrame.sample(3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GQ_C-WJxTud"
   },
   "source": [
    "### We can see that, non  of the columns has exact linear relationship. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDYnt0fsxTud",
    "outputId": "ae411eea-b912-4d7e-b8ff-a366855a7a2c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (17,10))\n",
    "sns.heatmap(data.corr(), annot = True, fmt = '.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o15eVBO3xTue"
   },
   "source": [
    "### We can see that, different columns of balance amount have the highest correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkW4PCmIxTue"
   },
   "outputs": [],
   "source": [
    "## Renaming the column\n",
    "data.rename(columns = {'default.payment.next.month':'def_pay'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgLRKCVaxTue",
    "outputId": "fd118423-e98c-4dee-ebc6-22f8efb204b0"
   },
   "outputs": [],
   "source": [
    "## Checking distribution of variables\n",
    "dnum = ['AGE', 'LIMIT_BAL', 'PAY_0', 'PAY_2', 'BILL_AMT1', 'BILL_AMT2', 'def_pay']\n",
    "\n",
    "plt.figure(figsize=(17,10))\n",
    "for i, column in enumerate(dnum, 1):\n",
    "    plt.subplot(2,4,i)\n",
    "    sns.distplot(data[column])\n",
    "    plt.xlabel(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgU9VdvexTue"
   },
   "source": [
    "### We can see that none of the columns follow normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fik0SPNHxTuf",
    "outputId": "47c703fa-0e56-4b03-e51f-62db0faefb6d"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data, columns = ['SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'def_pay'])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZ2le6y8xTuf"
   },
   "outputs": [],
   "source": [
    "X['SEX'] = X['SEX'].astype('object')\n",
    "X['EDUCATION'] = X['EDUCATION'].astype('object')\n",
    "X['MARRIAGE'] = X['MARRIAGE'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJgSsYv0xTui",
    "outputId": "e4d28c0f-0452-4634-b368-40353a530e9f"
   },
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sD3I2zrxTui",
    "outputId": "43c8a692-60db-4ae7-a094-49e4a179df15"
   },
   "outputs": [],
   "source": [
    "sex = {1:'MALE', 2: 'FEMALE'}\n",
    "ed = {1:'graduate school', 2:'university', 3:'high school', 4:'others', 5:'unknown'}\n",
    "mar = {1:'married', 2:'single', 3:'others'}\n",
    "X['SEX'].replace(sex, inplace = True)\n",
    "X['EDUCATION'].replace(ed, inplace = True)\n",
    "X['MARRIAGE'].replace(mar, inplace = True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PrIvHTWaxTuj",
    "outputId": "208b9775-e9d2-4d61-fad7-f2e6440d7b5c"
   },
   "outputs": [],
   "source": [
    "sns.barplot(x = 'def_pay', y ='AGE', data = X )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbEw-s0qxTuj"
   },
   "source": [
    "#### We can see that average age of defaulters is nearly equal to non - defaulters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYiD_SxUxTuj",
    "outputId": "b21bd3b6-8496-4c8b-ffb1-009a308abbc4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "sns.barplot(x = 'EDUCATION', y = 'AGE', hue = 'def_pay', data = X)\n",
    "plt.legend(loc = 'upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8E4wTIKxTuj"
   },
   "source": [
    "### We can observe that for both defaulters, and non - defaulters on a whole, the average age is less where education is others, and it is the highest where education is high school."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SO0EkCvBxTuk",
    "outputId": "9782e9db-bfbb-40e2-c1e7-31a79e030b8a"
   },
   "outputs": [],
   "source": [
    "## Cat plot\n",
    "sns.catplot(x = 'MARRIAGE', y = 'AGE', hue = 'def_pay', data  = X, kind = 'boxen', height=6, aspect = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AKam7bpxTuk"
   },
   "source": [
    "### We can observe that, median of age, for married people and others for both defaulters, and non - defaulters is nearly same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAhMJEaMxTuk"
   },
   "source": [
    "### As our dataset is imbalanced, we will use different Performance metrics. \n",
    "- In this I will use performance metric like Confusion Matrix and classification report, to conclude my findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rmv9vtvpxTuk"
   },
   "outputs": [],
   "source": [
    "### Seperating the dependent feature and independent features\n",
    "dep = data.drop(['def_pay'], axis = 1)\n",
    "indep = data['def_pay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xERZ1yeKxTuk",
    "outputId": "6a6e3d9e-8107-4bcb-c3fa-a500e8c5fce9"
   },
   "outputs": [],
   "source": [
    "dep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUVrBSBYxTuk",
    "outputId": "923ef1fd-00e1-47ca-c30b-fa3eb9b92c10"
   },
   "outputs": [],
   "source": [
    "indep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSBsUuaoxTul"
   },
   "outputs": [],
   "source": [
    "## Creating train and test dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dep, indep, test_size=0.3, random_state= 42, stratify=indep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1cqrMYixTul",
    "outputId": "b36924d5-7de9-41a9-86ea-171fd01af88e"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8woLKsDIxTul"
   },
   "source": [
    "### Feature Selection\n",
    "- ANOVA is a good measure, which is used when input variables are numerical and output variable is categorical in nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4Pf1rgMxTul"
   },
   "outputs": [],
   "source": [
    "## I am using ANOVA f-measure\n",
    "fs = SelectKBest(score_func=f_classif, k = 15)\n",
    "X_train_new = fs.fit_transform(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KJ9BJyUAxTum",
    "outputId": "d6fed2e8-464e-48cb-968c-d2cbc489c586"
   },
   "outputs": [],
   "source": [
    "X_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_qL-xGexTum",
    "outputId": "baead23b-7e71-4655-c4f0-db3de4ef2920"
   },
   "outputs": [],
   "source": [
    "## Checking the  scores for the features\n",
    "for i, feature in enumerate(X_train.columns, 0):\n",
    "\tprint('Feature %s: %f' % (feature, fs.scores_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwNMwNPaxTum"
   },
   "source": [
    "### The features I am considering are LIMIT_BAL, SEX, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, and PAY_AMT6 for building our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfRubieCxTum"
   },
   "outputs": [],
   "source": [
    "X_test_new = fs.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Scaling the data\n",
    "sc = StandardScaler()\n",
    "X_train_new = sc.fit_transform(X_train_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new = sc.transform(X_test_new)\n",
    "X_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('SC.pkl', 'wb')\n",
    "pickle.dump(sc, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYr6V0EnxTum"
   },
   "source": [
    "### Fitting the model \n",
    "- Here I am, using 2 models to compare namely, Logistic Regression and Random Forest. \n",
    "- Moreover, I am also applying Hyperparameter Tuning to my models with the help of pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXNxixSKxTum",
    "outputId": "82f2d675-1f8c-4686-a300-b9e226408eb4"
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('classifier', LogisticRegression)])\n",
    "\n",
    "grid_param = [\n",
    "            { 'classifier':[LogisticRegression()],\n",
    "              'classifier__penalty':['l2'],\n",
    "              'classifier__C':np.logspace(0, 4, 10),\n",
    "              'classifier__solver':['newton-cg','saga','sag','liblinear']                \n",
    "            },\n",
    "            {\"classifier\": [RandomForestClassifier()],\n",
    "                 \"classifier__n_estimators\": [10, 100, 1000],\n",
    "                 \"classifier__max_depth\":[5,8,15,25,30,None],\n",
    "                 \"classifier__min_samples_leaf\":[1,2,5,10,15,100],\n",
    "                 \"classifier__max_leaf_nodes\": [2, 5,10]\n",
    "                 }\n",
    "            \n",
    "]\n",
    "\n",
    "gridsearch = GridSearchCV(pipe, grid_param, cv = 5, verbose=1, n_jobs=-1 )\n",
    "best_model = gridsearch.fit(X_train_new, Y_train)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWRxBzpfxTun",
    "outputId": "e0260820-056a-47b9-faca-096766f5f754"
   },
   "outputs": [],
   "source": [
    "best_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that Random Forest Classifier is chosen as the best model with the specified values of parameters as given value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPhvEtmErr0m",
    "outputId": "97e5197f-9cbf-47a1-a815-1465e750ab09"
   },
   "outputs": [],
   "source": [
    "rf_pred = best_model.predict(X_test_new)\n",
    "rf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "sonaPaT0uCIH",
    "outputId": "8098d052-6dc3-4ef5-987f-912562d4e046"
   },
   "outputs": [],
   "source": [
    "cm_rf = confusion_matrix(Y_test, rf_pred)\n",
    "plt.figure(figsize = (8,8))\n",
    "sns.heatmap(cm_rf, annot = True, fmt = '.3f', square = True)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From this we can see the true positives values of different classes. To understand better, let's see classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RXu5x-shvg9n",
    "outputId": "d1daf2f1-81f4-4b6e-9753-31369ca563ec"
   },
   "outputs": [],
   "source": [
    "## Printing Classification Report \n",
    "print(classification_report(Y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From this we can observe that, the accuracy or our model is 81% which is good. Let's dive deeper into this, we observe that precision and recall values for class 0 are very good but on the other hand we can't say the same for class 1. My main aim in this project is to correctly classify class 1 values, which means I want to focus more on recall value for class 1. Moreover, F1-score for class 1 is very less, so let's see if we can increase this too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FiNjKRwzvv4e",
    "outputId": "b353c90d-18fc-43d6-e188-56918d37ab5f"
   },
   "outputs": [],
   "source": [
    "## We can see that the above model works good for class 0, but not that good for class 1. so we are trying to implement an another algorithm called XGBoost.\n",
    "grid_param2 = [\n",
    "            { 'classifier':[XGBClassifier()],\n",
    "              'classifier__learning_rate':[0.01, 0.1],\n",
    "              'classifier__max_depth':[3, 5, 7, 10],\n",
    "              'classifier__sub_sample':[0.5, 0.7]                \n",
    "            }\n",
    "]\n",
    "gridsearch = GridSearchCV(pipe, grid_param2, cv = 5, verbose=1, n_jobs=-1 )\n",
    "model2 = gridsearch.fit(X_train_new, Y_train)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2yhYEluazRWs",
    "outputId": "73cf027f-ed47-490c-f51e-d9142445a12d"
   },
   "outputs": [],
   "source": [
    "model2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above snippet, we can see that the Xgboost model works best with above parameter values. \n",
    "### Let's see its confusion matrix and classification report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "bGdGVUHi0poq",
    "outputId": "304c3e07-efa2-4bd2-a4f5-db2358a95c18"
   },
   "outputs": [],
   "source": [
    "xg_pred = model2.predict(X_test_new)\n",
    "cm_xg = confusion_matrix(Y_test, xg_pred)\n",
    "plt.figure(figsize = (8,8))\n",
    "sns.heatmap(cm_xg, annot = True, fmt = '.3f', square = True)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrGcFgfH04uZ",
    "outputId": "05257d9f-76ed-4ba4-b962-028165ed5365"
   },
   "outputs": [],
   "source": [
    "## Printing Classification Report \n",
    "print(classification_report(Y_test, xg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can observe that, this model works a little better than the Random Forest model as both recall and f1-score value of our model has increased. \n",
    "### Now I am trying to increase the recall value and f1-score much more. Hence, I am using a technique called Random Over Sampling to create more data points of class 1 in my training set. The ratio I am choosing is 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9WMHqFTt0-oe",
    "outputId": "39acfc7f-affe-4cbe-98f5-7fd2934f4a14"
   },
   "outputs": [],
   "source": [
    "## To improve it, we will do random oversampling and then compare different models\n",
    "os = RandomOverSampler(0.75)\n",
    "X_train_ns, Y_train_ns = os.fit_resample(X_train_new, Y_train)\n",
    "print(\"The number of classes before fit {}\".format(Counter(Y_train)))\n",
    "print(\"The number of classes after fit {}\".format(Counter(Y_train_ns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfNg1KcS6Jfq",
    "outputId": "0224c229-0316-4c10-a96a-eccbe2e721f3"
   },
   "outputs": [],
   "source": [
    "## First implementing RandomForest\n",
    "pipe2 = Pipeline([('classifier', RandomForestClassifier)])\n",
    "\n",
    "grid_param3 =[\n",
    "            {\"classifier\": [RandomForestClassifier()],\n",
    "                 \"classifier__n_estimators\": [10, 100, 1000],\n",
    "                 \"classifier__max_depth\":[5,8,15,25],\n",
    "                 \"classifier__min_samples_leaf\":[1,2,5,10,15],\n",
    "                 \"classifier__max_leaf_nodes\": [2, 5,10]\n",
    "                 }\n",
    "            \n",
    "]\n",
    "\n",
    "gridsearch = GridSearchCV(pipe2, grid_param3, cv = 5, verbose=1, n_jobs=-1 )\n",
    "model3 = gridsearch.fit(X_train_ns, Y_train_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5pOuoZGjHRhn",
    "outputId": "76d14551-9525-4afc-fc9c-45f45682e8ae"
   },
   "outputs": [],
   "source": [
    "model3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "J5qN0XYn7RwN",
    "outputId": "73ae1197-a4c3-4c16-8f83-2f388a6b9e1c"
   },
   "outputs": [],
   "source": [
    "rf_pred2 = model3.predict(X_test_new)\n",
    "cm_rf2 = confusion_matrix(Y_test, rf_pred2)\n",
    "plt.figure(figsize = (8,8))\n",
    "sns.heatmap(cm_rf2, annot = True, fmt = '.3f', square = True)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4A68jAlMHa1Y",
    "outputId": "e707be87-baa6-42be-915e-6887fdbd121b"
   },
   "outputs": [],
   "source": [
    "## Printing Classification Report \n",
    "print(classification_report(Y_test, rf_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After applying the sampling technique, we can see that the Random Forest Model works much better and now the recall and f1-score values for class 1 has also increased. Now I am trying to apply Xgboost to see if I can increase the values much further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgP9puqEHsO5",
    "outputId": "7ddf1bf6-33f2-4176-941b-08706d46f1c8"
   },
   "outputs": [],
   "source": [
    "grid_param4 = [\n",
    "            { 'classifier':[XGBClassifier()],\n",
    "              'classifier__learning_rate':[0.01, 0.1],\n",
    "              'classifier__max_depth':[3, 5, 7, 10],\n",
    "              'classifier__sub_sample':[0.5, 0.7]                \n",
    "            }\n",
    "]\n",
    "gridsearch = GridSearchCV(pipe, grid_param4, cv = 5, verbose=1, n_jobs=-1 )\n",
    "model4 = gridsearch.fit(X_train_ns, Y_train_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXsjkL8SIh9K",
    "outputId": "9a6ae5f1-b114-4cbd-b77c-3ca2ef353833"
   },
   "outputs": [],
   "source": [
    "model4.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "IY6xn_pWInqY",
    "outputId": "68a1d686-700e-40d3-bfb1-8ab5bbe76b3b"
   },
   "outputs": [],
   "source": [
    "xg_pred2 = model4.predict(X_test_new)\n",
    "cm_xg2 = confusion_matrix(Y_test, xg_pred2)\n",
    "plt.figure(figsize = (8,8))\n",
    "sns.heatmap(cm_xg2, annot = True, fmt = '.3f', square = True)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "STN9wbnbIzy0",
    "outputId": "2739d2ba-bac8-4a3c-e147-84b692976490"
   },
   "outputs": [],
   "source": [
    "## Printing Classification Report \n",
    "print(classification_report(Y_test, xg_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the obove figures, we can see that the Xgboost model is not performing better than the previous Random Forest Model.\n",
    "### After observing all the models I have created, I found out that Random Forest built after sampling technique performs much better than the rest. So, I will use that for deployement.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRwhEB3UJeuX"
   },
   "outputs": [],
   "source": [
    "### Doing pickling on model3 for deployement\n",
    "file = open('model3.pkl', 'wb')\n",
    "pickle.dump(model3, file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0AKam7bpxTuk"
   ],
   "name": "Credit Card code.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "e0c6c7c54332641693daf206df7c0799085a18d93134cf73d7f9518f5d9f4107"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
